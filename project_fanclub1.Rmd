---
title: "STAT 420: Final Data Project (OG Fan Club #1)"
authors: "Sarah Lee, Aayush Aggarwal, Albert Sadiku, Alexander Marcozzi"
date: "Due: Monday, December 14 by 11:30 PM CT"
output: 
  html_document: 
    theme: readable
    toc: yes
---

# Data Explaination and Setup

<br>

**Description of the Data:**

The data file used for this project focuses on pollution in the U.S. The file includes emission data from cities and towns around the country across multiple days and years, though we have chosen to focus on 2016 in the interest of workability. The variables include location information, such as state, city, and county, along with pollution information such as the amount of various greenhouse gasses emitted such as NO2, CO2, SO2, and O3. In total, there are 24 variables that actually contain data, and there are 6151 observations after cleaning duplicate entries and taking just the 2016 data. We will be attempting to create a model with CO.Mean as the response.

<br>

**Note about the data:**

The original dataset had two sets of duplicated measurements each day, the only difference being the CO measurements (the last 3 columns on the original data). The first set of duplicates had a value for ‘CO.AQI’ and the other duplicated set did not (marked as ‘NA’). To keep the data consistent, we removed the second duplicated set that had the 'NA' - ultimately, cleaning the dataset so there's only one recorded data point that's present per day. We will also remove certain unnecessary variables and entries that are missing data or/and are not needed for this analysis. 

<br>

**Import the data:**
```{r, message = FALSE, warning = FALSE}
#install.packages("lmtest")
library(faraway)  # for VIF function
library(MASS)     # for boxcox
library(lmtest)   # for shapiro normality test

pollution = read.csv('pollution_no_dup.csv')
pollution = subset(pollution, select = -c(X, State.Code, County.Code, Site.Num, Address, NO2.Units, O3.Units, SO2.Units, CO.Units))
pollution = pollution[complete.cases(pollution), ]  # remove entries that are missing data
names(pollution)
```

<br>

**Function Definitions:**
```{r}
plot_fitted_resid = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
  plot(fitted(model), resid(model), 
       col = pointcol, pch = 20, cex = 1.5,
       xlab = "Fitted", ylab = "Residuals")
  abline(h = 0, col = linecol, lwd = 2)
}

plot_qq = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
  qqnorm(resid(model), col = pointcol, pch = 20, cex = 1.5)
  qqline(resid(model), col = linecol, lwd = 2)
}
```


# Building the Model

**Check for multicollinearity:**

```{r}
numeric_data = subset(pollution, select = -c(State, County, City, Date.Local))
numeric_data = numeric_data[is.finite(rowSums(log(numeric_data[-1]))),]
pollution_add = lm(CO.Mean ~ ., data = numeric_data)

vif(pollution_add)
names(numeric_data)[vif(pollution_add) <= 5]
names(numeric_data)[vif(pollution_add) <= 10]
```

<br>

We can see that quite a few of our prospective predictor variables have large VIF values, which suggests multicolinearity. Our next step will be to compare the resulting smaller models with the model containing all predictors.

<br>

```{r}

pollution_add_less5 = lm(CO.Mean ~ NO2.1st.Max.Hour + O3.1st.Max.Hour + SO2.Mean + SO2.1st.Max.Hour + CO.1st.Max.Value, data = numeric_data)

pollution_add_less10 = lm(CO.Mean ~ NO2.Mean + NO2.1st.Max.Hour + O3.Mean + O3.1st.Max.Hour + SO2.Mean + SO2.1st.Max.Hour + CO.1st.Max.Value + CO.1st.Max.Value + CO.AQI, data = numeric_data)

anova(pollution_add_less5, pollution_add_less10)
anova(pollution_add_less10, pollution_add)

summary(pollution_add_less5)$adj.r.squared
summary(pollution_add_less10)$adj.r.squared
summary(pollution_add)$adj.r.squared
```

<br>

From the ANOVA tests as well as the adjusted R-squared values above, it seems that the multicollinearity is not a problem, so we will not drop any predictors.

<br>

**Trying Two-way Interactions:**

<br>

Now that we have made a decision on multicollinearity, we will try models with two-way interactions.

<br>

```{r}
pollution_two_way = lm(CO.Mean ~ . ^ 2, data = numeric_data)

anova(pollution_add, pollution_two_way)
```

```{r}
summary(pollution_two_way)$adj.r.squared
```

<br>

As we can see, the model using two-way interactions outperforms the previous best additive model. We can now try to use backwards AIC variable selection to make the model smaller.

<br>

```{r}
pollution_two_back_aic = step(pollution_two_way, direction = "backward", trace = 0)

anova(pollution_two_back_aic, pollution_two_way)

```


```{r}
summary(pollution_two_way)$adj.r.squared
summary(pollution_two_back_aic)$adj.r.squared
```

<br>

From the ANOVA test as well as the adjusted R-squared values, we can see that the model produced from the backwards AIC variable selection performs better than the previous best model with all two-way interactions.

```{r}
length(names(coef(pollution_two_way)))
length(names(coef(pollution_two_back_aic)))
```
Taking a look at the number of parameters, we have managed to reduce the number of predictors from **121** to **61** using the backwards AIC search method. 

We will now use the backwards BIC search method and identify if we can improve our two - way model further.

```{r}
n = length(resid(pollution_two_way))
pollution_two_back_bic = step(pollution_two_way, direction = "backward", trace = 0, k = log(n))
```

We will now compare this model to the one identified by the AIC search method.
```{r}
anova(pollution_two_back_bic, pollution_two_back_aic)
```
```{r}
summary(pollution_two_back_bic)$adj.r.squared
summary(pollution_two_back_aic)$adj.r.squared
```

From the above F - Test, we find that the $P - value = 0$. This means that for a reasonable confidence level of $\alpha = 0.05$, we reject the null hypothesis that that the predictors in the bigger AIC model are not significant. Furthermore, the Adjusted $R^2$ value for the AIC model is greater than the BIC model. Therefore, we lean towards the AIC model.

Our final step is to identify whether our AIC model is not over - fitting the data by comparing the **Cross - Validated RMSE** to the BIC model. 

```{r}
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

calc_loocv_rmse(pollution_two_back_aic)
calc_loocv_rmse(pollution_two_back_bic)
```
As we can see above, the Cross - Validated RMSE is lower for the AIC model. Therefore, the AIC model is also better at predicting future observations.

**Final Model**

Based on our analyses, we have decided to use the **2 - way interactive AIC model** as our final model for predicting **CO** levels.

<br>

**Testing assumptions (work in progress):**

```{r}
# shapiro.test(resid(pollution_add))
# bptest(pollution_add)
# plot_fitted_resid(pollution_add)
# 
# 
# 
# lambda = with(boxcox(pollution_add, plotit = FALSE), x[which.max(y)])
# lambda
# 
# pollution_add_2 = lm((((CO.Mean ^ lambda) - 1) / lambda) ~ ., data = numeric_data)
# plot_fitted_resid(pollution_add_2)
# summary(pollution_add_2)$adj.r.squared
# 
# bptest(pollution_add_2)
# shapiro.test(resid(pollution_add_2))
# 
# pollution_sqr = lm(CO.Mean ~ . ^ 2, data = numeric_data)
# plot_fitted_resid(pollution_sqr)
# anova(pollution_add, pollution_sqr)
```

<br>

**Trying Categorical Predictors:**

```{r}

```


<br>

**Finishing Touches?**

<br>

**Summary and Conclusion**

<br>


